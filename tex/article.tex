\documentclass[a4paper,10pt,twoside]{article}

% Note that if you want to use the \begin{equation} ... \end{equation}
% environment, you will have to include fleqn in the
% \documentclass[...]{...} options! 
% The top of your LaTeX file should then look like this:
% \documentclass[a4paper,10pt,twoside,fleqn]{article}

\usepackage{clin}        % Stylefile for CLIN Journal
\usepackage{harvard}     % Bibliography Stylefile
\usepackage{wrapfig}
\usepackage{footnote}
%\usepackage{savenotes}
%\usepackage{...,cgloss4e,avm,trees,tree-dvips,gb4e,ipa,graphicx}
                         % Whatever other packages you need
% Harvard:
% \cite{Covington}             (Covington 1994)
% \citeasnoun{Covington}       Covington (1994)
% \citeyear{Covington}         (1994)

\pagestyle{empty}

\begin{document}

\title{Using vector space models based on LDA and LSA for automatically grading exam questions}

%Authors and addresses
%Example has 3 authors with 2 different affiliations
%Adapt where necessary
\author{Johanna F. de Vos$^*$ \email{johannadevos@gmail.com}
\AND \addr{$^*$Radboud University, Donders Institute for Brain, Cognition and Behaviour, Nijmegen, The Netherlands}}


\maketitle\thispagestyle{empty} % extra pagestyle command for first page

%To be filled in by the editors
%Please leave commented out
%\jmlrheading{vol}{year}{pages}{Submission date}{Publication date}{authors}
%\copyright

\begin{abstract}
Students' answers to open-ended exam questions can be automatically graded by comparing them to a `perfect' reference answer in terms of semantic similarity. In this study, I compared three methods for constructing vector spaces in which semantic similarity can be measured. The dimensions of these vector spaces were made up of: 1) Topics, generated by LDA models, versus 2) Singular values, computed by LSA models, versus 3) Vocabulary counts, serving as baseline models. Better results were expected for the LDA and LSA models, as they capture students' answers at the semantic level rather than depending on exact vocabulary matches. Indeed, the best result was found for one of the LDA models, which achieved a Spearman's correlation coefficient of .52 between the grades predicted by the model and the true grades assigned by the lecturer. The LSA models on average outperformed the baseline models as well. In addition, it was found that the type of data used for training the LDA and LSA models impacted the accuracy of the predictions.
\end{abstract}

\section{Introduction}

Methods for automatically grading answers to exam questions have been developed since the 1960s. Initially, these approaches were based on structural features of student answers, such as the total number of words and the average sentence length (e.g., \citeasnoun{page1966}). In recent years, researchers' focus has shifted to content-based approaches. One such research avenue is based on the concept of semantic similarity: quantifying to what extent two documents are alike in terms of the meaning that they express.

For example, one can compare how similar a student's answer to an exam question is to the `perfect' answer to this question \cite{wolfe1998}, which I will call the reference answer. This is the approach that I followed in the current study. The basic idea is that that better student answers will resemble the reference answer more closely than not-as-good student answers. Measures of semantic similarity can be used to quantify how closely a student answer resembles the reference answer: high semantic similarity indicates that the student answer deserves a high grade.

The semantic similarity between two documents (here: a student answer and the reference answer) can be quantified by calculating the cosine of the angle between the two vectors that represent these documents in a semantic space. How the dimensions in this semantic space come about depends on the underlying model that is used to capture the meaning of a document. In this project, I focused on two such models: Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis (LSA).

LDA models \cite{blei2003} are generative topic models that take the form of a probability distribution over topics in a document collection. In turn, each topic takes the form of a probability distribution over words. LDA models are calculated from a document-term matrix. When using such a model to calculate semantic similarity between two documents, each dimension in the semantic space represents one topic. More specifically, for each document its vector representation consists of percentages that reflect the proportion of the document concerned with each topic. For example, if we have an LDA topic model with three topics [`Animals', `Science', `Sports'], the vector for a particular document could be [70, 23, 7].

LSA models \cite{landauer1997} are also topic models that are calculated from a document-term matrix, but they are conceptually and mathematically very different from LDA models. LSA reduces the dimensionality of the document-term matrix. Thus, in the semantic space, the dimensions will be those orthogonal components that are the result of the dimensionality reduction. They cannot be named or interpreted intuitively, like the topics in an LDA model. Nevertheless, we can calculate the distance between two document vectors in this new vector space to obtain the semantic similarity between two documents.

LDA and LSA models have previously been used for grading exam questions. Almost all of these studies focus on LSA (e.g., \citeasnoun{alves2015}, \citeasnoun{tobinski2013}, \citeasnoun{zhang2014}). To my knowledge, only one study \cite{kakkonen2008} has employed LDA for grading exam questions, comparing its effectiveness to that of LSA. \citeasnoun{kakkonen2008} expected LDA to outperform LSA, citing earlier information retrieval studies in which this was the case, but found the opposite. They explain this may be due to the small size of their training data (26, 42 and 70 student essays in three experiments).

The current study is motivated by the scarcity of studies on the use of vector space models in automatic grading in general, as well as by the question of whether \citeasnoun{kakkonen2008}'s training set impacted their results. To investigate the latter issue, I will make use of two different datasets for training, the first one being student answers (like in \citeasnoun{kakkonen2008}), and the second one being a chapter of a psychology textbook, which is more diverse in its contents. Thus, the research questions will be the following:

\begin{enumerate}
\item How effective are LDA-based and LSA-based vector space models for automatically grading students' answers to open exam questions?
\item Does the effectiveness of the above models depend on the training data that is used?
\item Are these topic models more effective than a simple vector space model based on vocabulary counts?
\end{enumerate}

\section{Approach}

\subsection{Data}
The core dataset consisted of 402 students' written answers to a first year Psychology exam question from Radboud University. The question was: ``Discuss Whorf's language theory. Include the following terms in your answer: Strong and weak variations on the theory." All answers were accompanied by the grade that the lecturer had assigned to that answer. Permission to work with these data was obtained through the ethical commission of the Social Science faculty. The student answers were used for two purposes: to train topic models (on the training set), and to predict grades from (on the test set). The total number of words was 23467.

As described in the introduction, I also used another text source to train the topic models. This was chapter 10 from \citeasnoun{gleitman2011}, the introductory psychology textbook that the students used. Chapter 10 is the chapter in which Whorf's language theory is explained. The chapter consisted of 1217 sentences (acting as documents in the document-term matrix) and a total of 15096 words.

The reference answer was based on a rubric that was provided by the lecturer, in combination with the definition of Whorf's language theory as given in \citeasnoun{gleitman2011}. It consisted of 59 words in 7 sentences.

\subsection{Preprocessing}
Pre-processing of the data (the student answers and the textbook) was done in Python (version 3.6) with the Natural Language Toolkit (NLTK) (version 3.2.4) \cite{bird2009}. This included tokenization, lemmatization and stop word removal. Manual spelling correction was applied to the student answers (see 2.5.2).

\subsection{Exploring parameters}
\label{sec:exploring}
In addition to comparing model types (baseline versus LDA versus LSA) and the influence of the training data (student answers versus textbook chapter), I also explored some other variables that could be expected to influence the accuracy of the model predictions.

\subsubsection{Counting method}
\label{sec:countingmethod}
I compared TF-IDF with \textsc{raw} counts, expecting better results for TF-IDF because it penalizes common words that do not distinguish between topics very well. TF-IDF was not implemented in the LDA models, which require integer counts due to being probabilistic models. I also looked at \textsc{binary} counting (0/1), as the student answers were relatively short and it is conceivable that the presence or absence of certain terms is already informative enough.

\subsubsection{Spelling correction}
If no spelling correction were to be applied, misspelled words would get their own vocabulary ID (e.g., `critizing' would be different from `criticizing'), leading to an unnecessary increase in the dimensionality of the dataset. The correct entry `criticizing' would have lower counts, and therefore might be assigned a less important role in the topic models than it should have had in reality. Therefore, I expected better outcomes when the data were spelling-corrected.

\begin{wrapfigure}{r}{5.5cm}
	\vspace{-15pt}
	\includegraphics[width=6cm]{"Histogram of grades"}
	\caption{Frequency distribution of the grades that were assigned by the lecturer (train split only).}
	\label{fig:histogram}
	\vspace{-45pt}
\end{wrapfigure} 

\subsubsection{Mapping algorithm}
Semantic similarity between the student answer and the reference answer was measured in terms of cosine similarity, which always lies between 0 and 1. The most straightforward way to transform this value into a grade is to multiply it by 10. I will call this mapping algorithm `\textsc{x10}'. However, Figure \ref{fig:histogram} shows that the grades of 1 and 9 were almost never assigned by the lecturer, but 0 and 10 quite often. Therefore, to try to improve prediction accuracy, the second mapping algorithm first multiplied the cosine similarity by 10, but subsequently mapped any 1s to 0, and any 9s to 10. I will call this mapping algorithm `\textsc{no 1 or 9}'.

\subsection{Implementation}

\subsubsection{Baseline}
The baseline models were vector space models whose dimensions corresponded to the words in the vocabulary of the test data and the reference answer. The way in which this vocabulary was counted is explained in section \ref{sec:countingmethod}. Thus, the baseline models were no topic models and were not trained on any data.

\subsubsection{Topic models}
The LDA models were instances of the LdaModel class in Python's \textit{gensim} library \cite{rehurek2010}, and the LSA models were instances of \textit{gensim}'s LsiModel class. The number of topics (in LDA) or singular values (SVs) (in LSA) needed to be specified beforehand. To find the optimal value for these hyperparameters, I performed a grid search on the training data (see section \ref{sec:training}).

\subsubsection{Training the models}
\label{sec:training}
The student answers were split into a 80/20 train/test set. Ten-fold cross-validation was applied to the training set in order to find the optimal number of topics and SVs. The models were evaluated by predicting grades for the answers in the validation subset of the training set (see section \ref{sec:evaluation}). I used this optimal number of topics or SVs to train the final topic models on all of the training data.

The textbook chapter was not split into a training and test set because it was only used for training topic models, and did not contain any grades that could be predicted. Again, various topic models were trained, differing in their number of topics or SVs. These models were evaluated by predicting grades for all student answers in the training set. Later, I used the best-performing models to predict grades on the test set.

As explained in section \ref{sec:exploring}, in this study I investigated/explored five different variables. The main variable under investigation was the model type (LDA versus LSA versus \textsc{baseline}). In addition, we were interested in the training data type (\textsc{student answers} versus a \textsc{textbook chapter}), the counting method (\textsc{raw} versus \textsc{binary} versus TF-IDF), the usage of spelling correction (\textsc{yes} versus \textsc{no}) and the mapping algorithm (\textsc{x10} versus \textsc{no 1 or 9}).

I trained models for all combinations of those five variables, with a few exceptions. Since the baseline models were not trained, for those models the training data type could not be manipulated. Furthermore, TF-IDF was not implemented in the LDA models, which require integer counts due to being probabilistic models. The LSA models could not be trained with TF-IDF when using the textbook data; for unknown reasons the Python kernel always shut down without outputting any error message. In the end, 48 different models were trained that represented all the other variable combinations.

\subsection{Evaluation}
\label{sec:evaluation}
The performance of all models was evaluated by correlating the grades predicted for the student answers in the test data with the grades that the lecturer had assigned to these answers. Spearman's correlation coefficient ($\textit{r}_s$) was used because the assigned and predicted grades often were not normally distributed.

\section{Results}

\subsection{Grid search: optimal number of topics / SVs}
The LDA models that were trained on the \textsc{student answers} on average performed best during ten-fold cross-validation when they contained only 2 topics ($\textit{r}_s$ = .39), as compared to models with 4 or 7 topics. I say `on average' because I averaged the outcomes of those models that varied in counting method, spelling correction and mapping algorithm. When training the LDA models on the \textsc{textbook chapter}, the average outcomes were quite different: the best-performing LDA models contained 20 topics ($\textit{r}_s$ = .49), as compared to 2, 4, 7, 12 or 40 topics.

The LSA models on average did best with 100 SVs ($\textit{r}_s$ = .38) when being trained on the \textsc{student answers}, as compared to 20, 50 or 200 SVs. When training them on the \textsc{textbook chapter}, 10 SVs were used in the best average performance ($\textit{r}_s$ = .35), as compared to 5, 20, 50, 100, 200 or 400 SVs.

Thus, in training the final topic models, I set the number of topics or SVs to the above-reported numbers that had yielded the best outcomes on the two types of training data.

\subsection{Outcomes per model type and other variables}
\label{sec:averageoutcomes}
Table \ref{table1} shows the average correlation that was obtained on the test data between predicted and lecturer-assigned grades, for each model type. The first column contains the four other variables under investigation. For each variable, the results in columns 3-5 are averaged over the levels (column 2) of the other three variables. For example, the results for the two types of training data (student answers vs. textbook chapter) are averaged over the different levels of counting method, spelling correction and mapping algorithm. It is possible that there could be (an) interaction(s) between the variables, but that was outside the scope of this study.

As can be seen from Table \ref{table1}, on average, the LDA models performed best, followed by the LSA models. The baseline models on averaged performed worst.

\begin{table}
\caption{Correlations for all model types and variables.}
\label{table1}
\centering
\begin{tabular}{lllll}
	\hline  &  & \textbf{Baseline} & \textbf{LDA} & \textbf{LSA} \\ 
	\hline  Training data & Student answers & N/A & .36 & .39 \\ 
						 & Textbook chapter & N/A & .42 & .32 \\ 
	\hline  Counting method & Raw & .38 & .38 & .34 \\ 
							& Binary & .34 & .40 & .35 \\ 
							& TF-IDF & .30 & N/A & .41 \\ 
	\hline  Spelling correction & Yes & .34 & .37 & .36 \\ 
								& No & .34 & .41 & .36 \\ 
	\hline  Mapping algorithm & x10 & .35 & .36 & .36 \\ 
							  & No 1 or 9 & .33 & .42 & .36 \\ 
	\hline  \textbf{Overall} &  & \textbf{.34} & \textbf{.39} & \textbf{.36} \\ 
	\hline 
	\end{tabular} 
\end{table}

\subsection{Best-performing individual models}
As explained in section \ref{sec:averageoutcomes}, all cells in Table \ref{table1} represent average outcomes.

The best-performing LDA model achieved a correlation of $\textit{r}_s$ = .52. It was trained on the textbook chapter, used binary counting, was spelling-corrected and used the x10 mapping algorithm. The scatterplot of predicted and assigned grades is shown in Figure 2.

\begin{figure}
\centering
\includegraphics[width=0.5\linewidth]{"Best LDA model"}
\caption{Grades predicted by the best LDA model.}
\label{lda}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{"Best LSA model"}
	\caption{Grades predicted by the best LSA model.}
	\label{lsa}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{"Best baseline model"}
	\caption{Grades predicted by the best baseline model.}
	\label{baseline}
\end{figure}

The best-performing LSA model achieved a correlation of $\textit{r}_s$ = .42. It was trained on non-spelling-corrected student answers and used binary counting. The mapping algorithm was not relevant, as no 1s and 9s were predicted to begin with. Figure 3 shows the scatterplot for this model.

Both of these models outperformed the best baseline model, which achieved a correlation of $\textit{r}_s$ = .38, using raw counts and spelling-corrected data. As in the LSA model, the mapping algorithm was irrelevant. The scatterplot is shown in Figure 4.

These three highest correlations all were significant (all \textit{p} $<$ .001).

\section{Discussion}
In this study I compared three types of vector space models for automatically predicting exam grades. The vector space models that used topics (LDA) or singular values (LSA) as their dimensions both outperformed the baseline vector space models that used vocabulary counts as their dimensions. The likely explanation for this is that LDA and LSA models compare documents at the semantic level, rather than looking for exact vocabulary matches \cite{kakkonen2008}. Notably, even in the baseline models the correlation between predicted and lecturer-assigned grades was statistically significant.

Looking back on the literature, \citeasnoun{kakkonen2008} asked whether their LSA models perhaps outperformed their LDA models because they were trained on a small set of student answers only. My results support this explanation: when training on the student answers LSA outperformed LDA, but it was the other way around when using a more diverse document collection (i.e., sentences in a psychology textbook chapter). This raises the question as to why the two types of training data affected the topic models differently (the size of the training sets was similar).

One explanation can be sought in the optimal number of topics for the LDA models that resulted from a grid search (see 2.3.3). This number was different for the two types of training data. In the student answers, the grid search had shown that the optimal number of topics was 2. This is the lowest possible number of topics that an LDA model can have. This outcome makes sense because in essence the student answers all revolved around the same topic (i.e., Whorf's language theory). As a result, however, the resulting vector space models only had two dimensions, which does not allow very fine distinctions between answers of different qualities. When visually inspecting the scatter plots of the outcomes of these models (not printed here), it can be seen that these models predict a 10 for almost all students, resulting in very low correlations. Thus, training on a more diverse corpus such as a textbook seems more suitable when working with LDA models. Indeed, Table 1 shows better results for LDA models that were trained on the textbook and used 20 topics, as compared to LDA models that were trained on student answers.

The textbook-trained LDA models also outperformed the textbook-trained LSA models by a large margin. This is the outcome that Kakkonen et al. (2008) had predicted in their own study. They list several arguments as to why LDA models are expected to function better, including the difficulty to select the right number of dimensions for LSA, the fact that no probability distribution is defined in LSA, and that the reduced matrix in LSA can contain negative values. These explanations likely also apply to the current study. Incidentally, it is nice to see the LDA model achieving the best results, because this is the model whose outcomes are easiest to interpret by students and lecturers.

Zooming in on the individual models, Figures 2-4 show that none of the models ever predicted the grade of 10, even though this was the most common grade in the training set (see Figure 1). Thus, while the lecturer considered many of the students’ answers to be excellent, these answers apparently were quite different from the reference answer in terms of the vector space models that I used for calculating semantic similarity. This shows that \cite{wolfe1998}'s reference answer approach may not be ideal for grading student answers by means of vector space models.

An alternative approach is described in \cite{foltz1998}, which may provide a better solution for future research. Rather than comparing a student's answer to one specific reference answer, the to-be-graded answer is compared to a set of answers already graded by the lecturer. Then, you take the (for example) ten graded answers with the highest similarity score to the to-be-graded answer. The predicted grade will be the weighted average of the grades of the ten most similar answers (weighted by their similarity score).

Another limitation of the current approach (and the alternative approach from \cite{foltz1998} as well) is that creative answers are punished, such as when a student uses a unique example. This makes the student’s answer more dissimilar from the reference answer (or from other students’ answers), and results in a lower grade. One solution could be to create a list with terms that are relevant to the question (e.g., [`Whorf', `linguistic', `relativity']) and first reduce the student’s answer to only the sentences that contain one of these terms. Sentences such as ‘The Inuit have many words for snow’ would no longer be part of the semantic similarity calculation, and perhaps wrongfully lower the predicted grade.

With regard to the three other variables, spelling correction and the mapping algorithm did not seem to have much of an effect in the baseline and LSA models. In the LDA models they did, but these models are the least reliable for evaluating these variables, because LDA models are probabilistic and their results were seen to fluctuate quite a bit when the same model was run twice. The baseline model worked best with raw counts; for LDA not much of a difference is seen, and for LSA it seems best to use TF-IDF counting.

The evaluation metric in this study was the correlation between the assigned and predicted grades. It would be interesting to consider other metrics too, for example the sum of squared errors (SSE) between the assigned and predicted grades. This would enable us to also evaluate a baseline model that would assign the most common grade (in this case 10) to all student answers. Such a baseline model currently could not be employed because a correlation coefficient cannot be calculated when there is no variance in the data.

The important question to close with is of course whether the models in this study could have any real-life implications. While it was good to see that all of the models yielded a statistically significant correlation between predicted and assigned grades, it still seems too early to justify their actual implementation in higher education. We would need to know what is the correlation between two human graders, and equal or go beyond that correlation with our model. Furthermore, the model needs to be able to assign the maximum grade of 10 to excellent answers that are different from the reference answer. With the suggestions proposed in this discussion, we should continue to try to find a reliable method for automatizing the grading process. This could be of great benefit to students, as they could be provided with feedback on their learning process much more quickly.




%\nocite{Sag} % items in your bibliography file that are not cited in your text

% add other references to the file bibliography.bib in this directory
\bibliographystyle{clin} 
\bibliography{bibliography} 

\section{Appendix}
\begin{table}[]
	\centering
	\footnotesize
	\caption{All models. The best correlation(s) per model type is/are highlighted in boldface.}
	\label{my-label}
	\begin{tabular}{lllllll}
		\hline model type & training data type & counting method & spelling correction & mapping algorithm & $\textit{r}_s$   & \textit{p} \\
		\hline baseline   & none               & raw             & yes                 & x10               & \textbf{.38} & .0005       \\
		baseline   & none               & raw             & no                  & x10               & .37 & .0006       \\
		baseline   & none               & raw             & yes                 & no 1 or 9         & \textbf{.38} & .0005       \\
		baseline   & none               & raw             & no                  & no 1 or 9         & .37 & .0006       \\
		baseline   & none               & binary          & yes                 & x10               & .33 & .0025       \\
		baseline   & none               & binary          & no                  & x10               & .36 & .0010       \\
		baseline   & none               & binary          & yes                 & no 1 or 9         & .33 & .0029       \\
		baseline   & none               & binary          & no                  & no 1 or 9         & .35 & .0013       \\
		baseline   & none               & TF-IDF          & yes                 & x10               & .34 & .0022       \\
		baseline   & none               & TF-IDF          & no                  & x10               & .33 & .0029       \\
		baseline   & none               & TF-IDF          & yes                 & no 1 or 9         & .27 & .0167       \\
		baseline   & none               & TF-IDF          & no                  & no 1 or 9         & .25 & .0244       \\
		LDA        & student answers    & raw             & yes                 & x10               & .28 & .0100       \\
		LDA        & student answers    & raw             & no                  & x10               & .21 & .0547       \\
		LDA        & student answers    & raw             & yes                 & no 1 or 9         & .43 & $<$ .0001       \\
		LDA        & student answers    & raw             & no                  & no 1 or 9         & .47 & $<$ .0001       \\
		LDA        & student answers    & binary          & yes                 & x10               & .26 & .0200       \\
		LDA        & student answers    & binary          & no                  & x10               & .29 & .0085       \\
		LDA        & student answers    & binary          & yes                 & no 1 or 9         & .45 & $<$ .0001       \\
		LDA        & student answers    & binary          & no                  & no 1 or 9         & .45 & $<$ .0001       \\
		LDA        & textbook chapter   & raw             & yes                 & x10               & .44 & $<$ .0001       \\
		LDA        & textbook chapter   & raw             & no                  & x10               & .45 & $<$ .0001       \\
		LDA        & textbook chapter   & raw             & yes                 & no 1 or 9         & .27 & .0134       \\
		LDA        & textbook chapter   & raw             & no                  & no 1 or 9         & .48 & $<$ .0001       \\
		LDA        & textbook chapter   & binary          & yes                 & x10               & \textbf{.52} & $<$ .0001       \\
		LDA        & textbook chapter   & binary          & no                  & x10               & .39 & .0004       \\
		LDA        & textbook chapter   & binary          & yes                 & no 1 or 9         & .30 & .0066       \\
		LDA        & textbook chapter   & binary          & no                  & no 1 or 9         & .51 & $<$ .0001       \\
		LSA        & student answers    & raw             & yes                 & x10               & .36 & .0011       \\
		LSA        & student answers    & raw             & no                  & x10               & .34 & .0021       \\
		LSA        & student answers    & raw             & yes                 & no 1 or 9         & .36 & .0011       \\
		LSA        & student answers    & raw             & no                  & no 1 or 9         & .34 & .0021       \\
		LSA        & student answers    & binary          & yes                 & x10               & .40 & .0003       \\
		LSA        & student answers    & binary          & no                  & x10               & \textbf{.42} & $<$ .0001       \\
		LSA        & student answers    & binary          & yes                 & no 1 or 9         & .40 & .0003       \\
		LSA        & student answers    & binary          & no                  & no 1 or 9         & \textbf{.42} & $<$ .0001       \\
		LSA        & student answers    & TF-IDF          & yes                 & x10               & .41 & .0001       \\
		LSA        & student answers    & TF-IDF          & no                  & x10               & .40 & .0002       \\
		LSA        & student answers    & TF-IDF          & yes                 & no 1 or 9         & .41 & .0001       \\
		LSA        & student answers    & TF-IDF          & no                  & no 1 or 9         & .40 & .0002       \\
		LSA        & textbook chapter   & raw             & yes                 & x10               & .32 & .0034       \\
		LSA        & textbook chapter   & raw             & no                  & x10               & .33 & .0029       \\
		LSA        & textbook chapter   & raw             & yes                 & no 1 or 9         & .33 & .0027       \\
		LSA        & textbook chapter   & raw             & no                  & no 1 or 9         & .32 & .0033       \\
		LSA        & textbook chapter   & binary          & yes                 & x10               & .29 & .0083       \\
		LSA        & textbook chapter   & binary          & no                  & x10               & .32 & .0040       \\
		LSA        & textbook chapter   & binary          & yes                 & no 1 or 9         & .29 & .0081       \\
		LSA        & textbook chapter   & binary          & no                  & no 1 or 9         & .32 & .0039 \\
		\hline     
	\end{tabular}
\end{table}





\end{document}
